\documentclass[12pt]{report}
\usepackage{graphicx} 
\usepackage{float} 
\usepackage{subfigure} 
\usepackage{titlesec}
\usepackage{amsfonts,amsmath,amssymb,amsthm,geometry,bbm,hyperref,nicefrac,enumerate,comment}
\usepackage{mathabx}


\titleformat{\chapter}[display]
  {\Huge\bfseries}{}{0pt}{\thechapter.\ }

\titleformat{name=\chapter,numberless}[display]
  {\Huge\bfseries}{}{0pt}{}
  

% \theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]

\newtheorem{example}[theorem]{Example} 
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{setting}[theorem]{Setting} 


\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} 

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newtheorem{mydef}{Definition}

\usepackage{bm}
\usepackage{mathrsfs}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\smallsum}{\textstyle\sum}
\renewcommand{\P}{\mathbb{P}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

\begin{titlepage}
   \begin{center}
       \LARGE
       \textbf{A Study of Data-driven Market Simulator}
       \\
       \vspace{2cm}
       \Large Master Thesis
 
       \vspace{1.5cm}

       \Large
       Songyan Hou\\
 
       \vfill

       
       \textit{\large Supervisor:}\\
       \large

       Prof. Dr. Josef Teichmann\\
       \normalsize
       Department of Mathematics\\
       ETH Z\"urich\\

       \vspace{0.8cm}
       Submitted: \today
 
   \end{center}
\end{titlepage}

\begin{abstract}
  asd
\end{abstract}

\tableofcontents
\chapter{Introduction}

\chapter{Financial times-series stimulation}
\section{Challenges in financial scenario}
\section{Stylised facts and evaluation of similarity}
\section{Classical and modern model}

\chapter{Signature feature}
In this chapter, we give a brief introduction to the signature feature of a path and some variation of signature feature such as log-signature, low-rank-signature and random-signature. Moreover, we discuss the expectation of these signature based features of a stochastic process and emphasize their characteristic ability of the law of stochastic process. For simplicity and readability, we restrict our introduction to the space of continuous bounded variation paths from a compact time interval $J$ to a Euclidean space $E = \R^{d}$ denoted by $\mathcal{C}^{1}_{0}(J,E)$. We postpone the discussion of semi-martingales and rough paths taking values in Banach space $E$ to the appendix and kindly invite interested readers to the comprehensive and rigorous introduction of signature feature in \cite{friz2020course} \cite{lyons2007differential}.  

\section{Signature}
We start the introduction from the tensor algebra in which our signature feature takes value. We consider the non-commuting formal power series of tensors with formal indeterminates as basis of $E$. 
\begin{definition}[Formal power series]
  We denote $T((E))$ the space of formal power series of tensors in $E$ that 
  \begin{equation*}
    \begin{split}
      T((E)) &= \Bigg\{\mathbf{a} = (\mathbf{a}_{k})_{k\geq 0} \colon \mathbf{a}_{k} \in E^{\otimes k}\Bigg\} = \bigoplus_{m\geq 0} E^{\otimes m} \\
    \end{split}
  \end{equation*}
  endowed with addition and multiplication defined as follows: Let $\mathbf{a} = (\mathbf{a}_{k})_{k\geq 0}, \mathbf{b} = (\mathbf{b}_{k})_{k\geq 0} \in T((E))$ and $\lambda \in \R$. Then 
\begin{equation*}
  \begin{split}
    \mathbf{a} + \mathbf{b} &= (\mathbf{a}_{k} + \mathbf{b}_{k})_{k\geq 0}\\
    \mathbf{a} \otimes \mathbf{b} &= \big(\sum_{i+j = k}\mathbf{a}_{i} \otimes \mathbf{b}_{j}\big)_{k\geq 0}\\
    \lambda\mathbf{a} &= (\lambda \mathbf{a}_{k})_{k\geq 0}
  \end{split}
\end{equation*}
Let $T^{(n)}(E)$ denote the truncated tensor algebra space up to order $n$
\begin{equation*}
  T^{(n)}(E) = \bigoplus_{k\leq n} E^{\otimes k}
\end{equation*}  
\end{definition}
Let $e_{1},\cdots,e_{d}$ be a finite basis of $E = \R^{d}$. Then $\mathbf{a}\in T((E))$ has the following linear form
\begin{equation*} 
  \mathbf{a} = \sum_{k\geq 0}\Big(\sum_{i_{1},\cdots,i_{k} = 1}^{d} a_{i_{1},\cdots,i_{k}}e_{i_{1}}\otimes\cdots\otimes e_{i_{k}} \Big),\quad a_{i_{1},\cdots,i_{k}} \in \R.
\end{equation*}
and $T^{(n)}(E)$ can be considered as a subspace of $T((E))$.
% and $T((E))$ is endowed with inner product naturally inherited from $\R^{d}$. 
% \begin{equation*}
%   \langle e_{i_{1}}\otimes\cdots\otimes e_{i_{k}}, e_{j_{1}}\otimes\cdots\otimes e_{j_{k}} \rangle_{T((E))} = \delta_{i_{1},j_{1}}\cdots \delta_{i_{k},j_{k}}
% \end{equation*} 

\begin{definition}
  We denote $\mathbf{T}(E)$ the Banach space 
  \begin{equation*}
    \mathbf{T}(E) \coloneq \bigg\{\mathbf{t} \in T((E)) \colon \lVert \mathbf{t}\rVert_{\mathbf{T}(E)} \coloneq \sqrt{\sum_{k\geq 0}\lVert \mathbf{t}_{k}\rVert_{E^{\otimes k}}^{2} } < \infty \bigg\}.
  \end{equation*} 
  Similarly, we denote $\mathbf{T}^{(n)}(E)$ the truncation of $\mathbf{T}(E)$ up to order $n$.
\end{definition}

\begin{definition}[Signature]
  We denote $\mathbf{Sig}_{J} \colon \mathcal{C}^{1}_{0}(J,E) \to \mathbf{T}(E)$ the signature map such that for all $X \in \mathcal{C}^{1}_{0}(J,E)$
  \begin{equation*}
    \mathbf{Sig}_{J}(X) = (1, \mathbf{s}_{1},\cdots) \in \mathbf{T}(E)
  \end{equation*}
  where 
  \begin{equation*}
    \begin{split}
      \mathbf{s}_{k} &= \int_{t_{1}<\cdots< t_{k} \in J}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{k}}\\
      &= \sum_{i_{1},\cdots,i_{k} = 1}^{d} \int_{t_{1}<\cdots< t_{k} \in J}dX_{t_{1}}^{i_{1}}\cdots dX_{t_{k}}^{i_{k}} \cdot e_{i_{1}}\otimes\cdots\otimes e_{i_{k}} \\
    \end{split}
  \end{equation*}
  Let $\mathbf{Sig}^{(n)}_{J}$ denote the truncated signature map up to order $n$
  \begin{equation*}
    \mathbf{Sig}^{(n)}_{J}(X) = (1, \mathbf{s}_{1},\cdots, \mathbf{s}_{M}) \in \mathbf{T}^{(n)}(E).
  \end{equation*}
\end{definition}
\begin{example}\label{linear}
  Let $X_{t} = t\mathbf{x} \in \R^{d}$, then 
  \begin{equation*}
    \mathbf{Sig}_{[0,1]}(X) = (1, \mathbf{x},\frac{\mathbf{x}^{\otimes 2}}{2!}, \cdots) \in \mathbf{T}(E)
  \end{equation*}
\end{example}

\subsection{Signature map}
We expect a good feature map to capture important information while ignoring irrelevant ones. First observation is the the signature feature is invariant of starting point because $d(X_{t} - X_{0}) = dX_{t}$. Moreover, it is invariant of reparametrization.
\begin{proposition}[Invariant under reparametrization]
  Let $X \in \mathcal{C}^{1}_{0}([S_{1},T_{1}],E)$ and  $\tau\colon [S_{1},T_{1}] \to [S_{2},T_{2}]$ a non-decreasing surjective reparametrization. Then 
  \begin{equation}
    \mathbf{Sig}_{[S_{2},T_{2}]}(X_{\tau(\cdot)}) = \mathbf{Sig}_{[S_{1},T_{1}]}(X) 
  \end{equation}
\end{proposition}
\begin{proof}
  \begin{equation*}
    \begin{split}
      \mathbf{Sig}_{[S_{2},T_{2}]}(X_{\tau(\cdot)})_{k} &= \int_{\tau(t_{1})<\cdots< \tau(t_{k}) \in [S_{2},T_{2}]}dX_{\tau(t_{1})}\otimes\cdots\otimes dX_{\tau(t_{k})}\\
      &= \int_{t_{1}<\cdots< t_{k} \in [S_{1},T_{1}]}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{k})} = \mathbf{Sig}_{[S_{1},T_{1}]}(X) 
    \end{split}
  \end{equation*}
\end{proof}
From the invariant property of reparametrization, we notice that signature map is not injective. However, signature feature is injective up to tree-like equivalence $\sim_{t}$ which we detailed in appendix, and obviously time-reparametrization is included in tree-like equivalence. We define $\mathcal{P}_{0}^{1} = \mathcal{C}^{1}_{0}([0,T],E) / \sim_{t}$ the quotient space up to tree-like equivalence endowed with quotient metric.  
\begin{theorem}[Weak uniqueness \cite{boedihardjo2016signature}] 
  $\mathbf{Sig}_{[0,T]}$ is injective on $\mathcal{P}_{0}^{1}$.
\end{theorem}
\begin{proof}
  See ???
\end{proof}
If we add time as an additional strictly increasing coordinate into path ie. $\overline{X_{t}} = (t,X_{t}) \in \overline{E} \coloneq \R\oplus E$, then $\mathbf{Sig}_{[0,T]}$ is injective on the time-augmented space. 
\begin{corollary}[Uniqueness \cite{boedihardjo2016signature}] 
  $\mathbf{Sig}_{[0,T]}$ is injective on $\mathcal{C}^{1}_{0}([0,T],\overline{E})$.
\end{corollary}
\begin{proof}
  Since the first coordinate is strictly increasing, the only tree-like equivalent path is itself i.e. $\mathcal{P}^{1}_{0} = \mathcal{C}^{1}_{0}([0,T],\overline{E})$, which concludes the proof. 
\end{proof}
\begin{proposition}\label{surjective}
  $\mathbf{Sig}_{[0,1]}$ is neither surjective nor the range of which a linear subspace of $\mathbf{T}(E)$.
\end{proposition}
\begin{proof}
  Let $X \in \mathcal{C}^{1}_{0}([0,T],E)$ and w.l.o.g assume $X_{0} = 0$. 
  By integration by part
  \begin{equation*}
    \begin{split}
      \mathbf{Sig}_{[0,1]}(X)_{1,2} + \mathbf{Sig}_{[0,1]}(X)_{2,1} &=  \int_{t_{1}<t_{2} \in [0,1]}dX_{t_{1}}^{1}dX_{t_{2}}^{2} + \int_{t_{1}<t_{2} \in [0,1]}dX_{t_{1}}^{1}dX_{t_{2}}^{2}\\
      &=  \int_{t \in [0,1]}d(X_{t}^{1}X_{t}^{2}) = \mathbf{Sig}_{[0,1]}(X)_{1}~\mathbf{Sig}_{[0,1]}(X)_{2}.
    \end{split}
  \end{equation*}
  Thus, $\mathbf{Sig}_{[0,1]}$ is neither surjective nor the range of it a linear subspace of $\mathbf{T}(E)$.
\end{proof}

\begin{theorem}[Weak universality]
  Let $A$ be a compact set of $\mathbf{Sig}(\mathcal{C}^{1}_{0}(J,E))$, then for all $f \colon A \to \R$ continuous and for all $\epsilon > 0$, there exists a linear functional $L \in \mathbf{T}(E)^{*}$ such that 
  \begin{equation}
    \sup_{\mathbf{a}\in A}\lVert f(\mathbf{a}) - L(\mathbf{a})\rVert \leq \epsilon
  \end{equation}
\end{theorem}
\begin{proof}
  Proof relies on the Stone-Weierstrass theorem and the shuffle product property of the signature detailed in appendx, see \cite{liao2019learning}.
\end{proof}


\subsection{Signature stream}
Instead of viewing signature as a static object, we can consider signature stream of a path $X \in \mathcal{C}^{1}_{0}([0,T],E)$ as a process $\big(\mathbf{Sig}_{[0,t]}(X)\big)_{[0,T]}$ taking values in $\mathbf{T}(E)$.
\begin{proposition}
  Let $X \in \mathcal{C}^{1}_{0}([0,T],E)$ and define $\pi_{n} \colon \mathbf{T}(E) \to \mathbf{T}^{(n)}(E)$ the projection such that for all $\mathbf{x} \in \mathbf{T}(E)$ 
  \begin{equation}\label{projection}
    \pi_{n}\big((\mathbf{x}_{k})_{k\geq 0}\big) = (\mathbf{x}_{k})_{k\leq n}
  \end{equation}
  then $S_{t} = \mathbf{Sig}^{(n)}_{[0,t]}(X)$ satisfies for all $t \in [0,T]$ that 
  \begin{equation}\label{SDE_signature}
    dS_{t} = \pi_{n}(S_{t}\otimes dX_{t}),\quad S_{0} = (1,0,\cdots),
  \end{equation} 
  and moreover $(S_{t})_{t \in [0,T]}$ is the unique solution of \eqref{SDE_signature}. 
\end{proposition}
\begin{proof}
  See Lemma 2.10 in \cite{lyons2007differential}.
\end{proof}


\begin{definition}
  Let $X \in \mathcal{C}^{1}_{0}([0,s],E)$ and  $X \in \mathcal{C}^{1}_{0}([s,t],E)$. The concatenated path $X\star Y \in\mathcal{C}^{1}_{0}([0,t],E) $ is defined by 
  \begin{equation}
    (X\star Y)_{u} = \left\{\begin{aligned}
      &X_{u} &u\in [0,s]\\
      &Y_{u} + (X_{s} - Y_{s}) &u\in [s,t]
    \end{aligned}\right.
  \end{equation}
\end{definition}
\begin{theorem}[Chen's identity]
  Let $X \in \mathcal{C}^{1}_{0}([0,s],E)$ and  $Y \in \mathcal{C}^{1}_{0}([s,t],E)$. Then 
  \begin{equation}
    \mathbf{Sig}_{[0,t]}(X\star Y) = \mathbf{Sig}_{[0,s]}(X) \otimes \mathbf{Sig}_{[s,t]}(Y)
  \end{equation}
\end{theorem}
\begin{proof}
  See Theorem 2.9 in \cite{lyons2007differential}.
\end{proof}

\begin{example}\label{linear_chern}
  Let $X$ be linear on $[n,n+1]$ and let $X_{n+1} - X_{n} = \mathbf{x}_{n}$ for $n \in \N$, then 
  \begin{equation*}
    \mathbf{Sig}_{[0,N]}(X) = \bigotimes_{n \leq N}(1, \mathbf{x}_{n},\frac{\mathbf{x}_{n}^{\otimes 2}}{2!}, \cdots) 
  \end{equation*}
\end{example}


\begin{example}[Linear controlled differential equation]
  Let $E = \R^{d}, W = \R^{n}$. let $X \in \mathcal{C}^{1}_{0}([0,T],E)$ and let $B\colon E \to \mathbf{L}(W)$ be a bounded linear map. Consider 
  \begin{equation}
    dY_t = B(dX_{t})(Y_{t}),\quad Y_{0} \in W  
  \end{equation}
  If we denote $B^{k} \coloneq B(e_{k})$, $k = 1,\cdots, d$ then 
  \begin{equation}\label{linear_control}
    dY_t = \sum_{k=1}^{d}B^{k}(Y_{t})dX^{k}_{t},\quad Y_{0} \in W .
  \end{equation}
  It follows from Picard's iteration that 
  \begin{equation}
    \begin{split}
      Y^{n}_{t} &= \Bigg(I + \sum_{k=1}^{n}B^{\otimes k}\int_{t_{1}<\cdots< t_{k} \in [0,t]}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{k}}\Bigg) Y_{0}\\ 
      &= \Bigg(I + \sum_{k=1}^{n}\sum_{i_{1},\cdots,i_{k} = 1}^{d}B^{i_{k}}\cdots B^{i_{1}}\int_{t_{1}<\cdots< t_{k} \in [0,t]}dX_{t_{1}}^{i_{1}}\cdots dX_{t_{k}}^{i_{k}}\Bigg) Y_{0}.
    \end{split}
  \end{equation}
  Let the variation of $X \in \mathcal{C}^{1}_{0}([0,T],E)$ denoted by $\lVert X \rVert_{[0,T]}$, then 
  \begin{equation}
      \Big\lVert \int_{t_{1}<\cdots< t_{k} \in [0,t]}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{k}} \Big\rVert_{E^{\otimes k}} \leq \frac{\lVert X \rVert_{[0,T]}^{k}}{k!}.
  \end{equation}
  Therefore, $Y^{n}_{t}$ converges to $Y_{t}$ as $n \to \infty$ i.e. 
  \begin{equation}
    \lVert Y_{t} - Y^{n}_{t}\rVert_{W} \leq \sum_{k > n}\frac{\lVert B\rVert_{\mathcal{L}(E,\mathcal{L}(W))}^{k}\lVert X \rVert_{[0,T]}^{k}}{k!}  \leq \frac{\lVert B\rVert_{\mathcal{L}(E,\mathcal{L}(W))}^{n+1}\lVert X \rVert_{[0,T]}^{n+1}}{n!} \to 0,\quad \text{as } n \to \infty
  \end{equation}
  and
  \begin{equation} 
    Y_{t} = \Bigg(I + \sum_{k=1}^{\infty}B^{\otimes k}\int_{t_{1}<\cdots< t_{k} \in [0,t]}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{k}}\Bigg) Y_{0}.
  \end{equation}
  In the language of signature 
  \begin{equation} 
    Y_{t} = \Big(\sum_{k=0}^{\infty}B^{\otimes k}\Big) \Big(\mathbf{Sig}_{[0,t]}(X)\Big) Y_{0}.
  \end{equation}
  which implies that the solution of controlled SDE could be written as a linear function on signature stream of control path. This implies that signature stream is a promising feature for controlled ODE.
\end{example}
\begin{remark}
  Similar result holds for smooth vector field with some additional boundedness assumption and the proof is in alignment with the argument above, see \cite{liao2019learning}. 
\end{remark}
Despite nice properties as feature map, signature feature suffers from dimension explosion w.r.t truncation order. Consider a path $X \in \mathcal{C}^{1}_{0}([0,T],\R^{d})$ and the truncated signature of $X$ up to order $n$ in $\mathbf{T}^{(n)}(\R^{d}) \subseteq \bigoplus_{k\leq n} (\R^{d})^{\otimes k} $ which has dimension $1 + d + \cdots + d^{n} = \frac{d^{n+1} - d}{d-1}$ growing exponentially w.r.t the truncation order $n$. Therefore, we would like to explore some low-rank approximations of signature feature. 
\section{Log-signature}
From Proposition \ref{surjective}, we know that $\mathbf{Sig}_{[0,1]}$ is neither surjective nor the range of which a linear subspace of $\mathbf{T}(E)$. In Example \ref{linear}, for $X_{t} = t\mathbf{x} \in \R^{d}$, then 
  \begin{equation*}
    \mathbf{Sig}_{[0,1]}(X) = \sum_{k\geq 0}\frac{\mathbf{x}^{\otimes k}}{k!}
  \end{equation*}
  which is the power series expansion of exponential function. Motivated by this, we curve the signature space by taking `logarithm' which we defined below. 
\begin{definition}
  Define $\mathbf{T}_{1}(E) = \{\mathbf{a} \in \mathbf{T}(E)\colon \mathbf{a}_{0} = 1\}$. Let $\mathbf{a} \in \mathbf{T}_{1}(E)$, we define the exponential be 
  \begin{equation*}
    \mathbf{exp}(\mathbf{a}) = \sum_{n\geq 0}\frac{\mathbf{a}^{\otimes n}}{n!}.
  \end{equation*}
  and we define the logarithm be
  \begin{equation*}
    \mathbf{log}(\mathbf{a}) = \mathbf{log}(1+\mathbf{t}) = \sum_{n\geq 1} \frac{(-1)^{n-1}}{n}\mathbf{t}^{\otimes \mathbf{n}}.
  \end{equation*}
\end{definition}
\begin{lemma}
  $\mathbf{exp}(\cdot)$ and $\mathbf{log}(\cdot)$ are inverse of each other on $\mathbf{T}_{1}(E)$.
\end{lemma}
\begin{proof}
  See Lemma 2.21 in \cite{lyons2007differential}.
\end{proof}
Therefore, taking logarithm of signature loses no information.
\begin{definition}[Log-signature]
  We denote $\mathbf{LogSig}_{J} \colon \mathcal{C}^{1}_{0}(J,E) \to \mathbf{T}(E)$ the log-signature map such that for all $X \in \mathcal{C}^{1}_{0}(J,E)$
  \begin{equation*}
    \mathbf{LogSig}_{J}(X) = \mathbf{log}\big(\mathbf{Sig}_{J}(X)\big)
  \end{equation*}
  Let $\mathbf{LogSig}^{(n)}_{J}$ denote the truncated signature map up to order $n$
  \begin{equation*}
    \mathbf{LogSig}^{(n)}_{J}(X) = \pi_{n}\big(\mathbf{LogSig}_{J}(X)\big)
  \end{equation*}
  where $\pi_n$ define in \eqref{projection}.
\end{definition}
\subsection{Lie Series}
Besides storing the same information, thanks to the shuffle product property of signature, log-signature space is linear and admits a more concrete representation. Precisely speaking, log-signature space is a linear subspace of Lie formal series over $E$. Recall the Lie bracket $[\cdot, \cdot]$ on $T((E))$ such that 
\begin{equation*}
  [\mathbf{a},\mathbf{b}] = \mathbf{a}\otimes\mathbf{b} - \mathbf{b}\otimes\mathbf{a}
\end{equation*}
If $F_{1}$ and $F_2$ are two linear subspaces of $T((E))$, let us denote by $[F1, F2]$ the linear span of all the elements of the form $[\mathbf{a}, \mathbf{b}]$, where $\mathbf{a} \in F_{1}$ and $\mathbf{b} \in F_{2}$.
\begin{definition}[Lie formal series]
  We denote $\mathcal{L}((E))$ the space of Lie formal series over $E$ such that 
  \begin{equation*}
    \mathcal{L}((E)) = \{\mathbf{l} \in T((E)) \colon \mathbf{l}_{n} \in L_{n}\}
  \end{equation*} 
  where 
  \begin{equation*}
    L_{0} = 0, L_{1} = [E,E], L_{2} = [E,L_{1}], \cdots ,L_{n} = [E,L_{n-1}], \cdots
  \end{equation*}
  and denote $\mathcal{L}^{(n)}(E) = \pi_{n}(\mathcal{L}((E)))$ the truncated Lie series up to order $n$.
\end{definition}
\begin{theorem}
  The range of log-signature is a linear subspace of Lie series over $E$.
  \begin{equation*}
    \mathbf{LogSig}(\mathcal{C}_{0}^{1})\subseteq\mathcal{L}((E)).
  \end{equation*}
\end{theorem}
\begin{proof}
  See Theorem 2.23 in \cite{lyons2007differential}.
\end{proof}
\begin{theorem}
  The range of truncated log-signature is the truncated Lie series over $E$ up to the same order
  \begin{equation*}
    \mathbf{LogSig}^{(n)}(\mathcal{C}_{0}^{1}) = \mathcal{L}^{(n)}(E)
  \end{equation*}
\end{theorem}
\begin{proof}
  See Proposition 2.27 in \cite{lyons2007differential}.
\end{proof}
With Lie series form, we compute the dimension of truncated log-signature space. 
\begin{proposition}
  The dimension of the space of truncated log-signature up to order $n$ is 
  \begin{equation*}
    w(d,n) = \sum_{k=1}^{n}\frac{1}{k}\sum_{i|k}\mu\Big(\frac{k}{i}\Big)d^{i}
  \end{equation*}
  which is the Witt's formula and $\mu$ is the M\"{o}bius function.
\end{proposition}
\begin{proof}
  See Corollary 4.14 in \cite{reutenauer2003free}.
\end{proof}
If we let $d=5$ and $n=5$, then $\dim(\mathbf{Sig}) = 3905$ while $\dim(\mathbf{LogSig}) = 829$. In summary, log-signature curves the signature space and reduces the redundancy without losing any information. However, there is no free lunch because log-signature loses the universality of signature, therefore requiring nonlinear models.  
\begin{example}
  For $X_{t} = t\mathbf{x} \in \R^{d}$, then 
  \begin{equation*}
    \mathbf{Sig}_{[0,1]}(X) = \mathbf{exp}(\mathbf{x}),\quad \mathbf{LogSig}_{[0,1]}(X) = \mathbf{x}.
  \end{equation*}
  Log-signature pick non-redundant information in signature.  
\end{example}
\section{Universality and characteristics }
Recall the weak universality of signature, we restrict ourselves on a compact set of $\mathbf{Sig}(\mathcal{C}^{1}_{0}(J,E))$. To generalize the theorem beyond compactness, we would like to normalize the set $\mathbf{Sig}(\mathcal{C}^{1}_{0}(J,E))$ into a bounded ball. The first idea comes to mind is to normalize signatures by scaling them on $\mathbf{T}(E)$. However, because signature map is neither surjective nor the range of it a linear subspace, scaled tensor of a signature might not be signature. Thus, we drive ourselves out of the region of $\mathbf{Sig}(\mathcal{C}^{1}_{0}(J,E))$ where we have the shuffle product property. An alternative is to normalize the signature by scaling the path. For $X \in \mathcal{C}^{1}_{0}(J,E)$ and $\lambda > 0$, the signature of the scaled path
\begin{equation*}
  \begin{split}
    \mathbf{Sig}_{J}(\lambda X)_{k} &= \int_{t_{1}\leq\cdots\leq t_{k} \in J}d\lambda X_{t_{1}}\otimes\cdots\otimes d\lambda  X_{t_{k}}\\ 
    &= \lambda^{k}\int_{t_{1}\leq\cdots\leq t_{k} \in J}dX_{t_{1}}\otimes\cdots\otimes dX_{t_{m}} = \lambda^{k}\mathbf{Sig}_{J}(X)_{k}
  \end{split}
\end{equation*}
Thus, the norm of the scaled path
\begin{equation*}
  \begin{split}
    \big\lVert \mathbf{Sig}_{J}(\lambda X)\big\rVert_{\mathbf{T}(E)}^{2} &= \sum_{k\geq 0} \lambda^{2k} \lVert \mathbf{Sig}_{J}(X)_{k}\rVert_{E^{\otimes k}}^{2} \\
    % &\neq \lambda \sqrt{\sum_{k\geq 0}\lVert \mathbf{Sig}_{J}(X)_{k}\rVert_{E^{\otimes k}}^{2} } = \lambda\big\lVert \mathbf{Sig}_{J}( X)\big\rVert_{\mathbf{T}(E)}\\
  \end{split}
\end{equation*}
Therefore, we define the scaling map $\delta_{\lambda}$ on $\mathbf{T}(V)$ such that for all $\mathbf{a} \in \mathbf{T}(V)$ 
\begin{equation*}
  \delta_{\lambda}(\mathbf{a}) = (\lambda^{k} \mathbf{a}^{k})_{k\geq 0}
\end{equation*} 
\begin{definition}
  A tensor normalization is a continuous injective map 
  \begin{equation*}
    \Lambda\colon \mathbf{T}(V) \to \{\mathbf{t} \in \mathbf{T}(V)\colon \lVert\mathbf{t}\rVert_{\mathbf{T}(E)} \leq K\}
  \end{equation*}
  \begin{equation*}
    \mathbf{t} \mapsto \delta_{\lambda(\mathbf{t} )}(\mathbf{t} )
  \end{equation*}
  where $K > 0$ and $\lambda\colon \mathbf{T}(V) \to (0,\infty)$ a function. 
\end{definition}
The existence of tensor normalization is not trivial because of the nonlinear relationship between the scaling factor $\lambda(\mathbf{t})$ and the norm of scaled tensor $\lVert \delta_{\lambda(\mathbf{t} )}(\mathbf{t} ) \rVert $ shown above. The proof of existence and the construction methodology can be found in the appendix. 
\begin{theorem}[\cite{chevyrev2018signature}]\label{moment_law}
  Let $\Lambda \colon \mathbf{T}(E) \to \mathbf{T}(E)$ be a tensor normalization. The normalized signature 
  \begin{equation*}
    \Phi = \Lambda \circ \mathbf{Sig}_{J}
  \end{equation*}
  \begin{enumerate}[(i)]
    \item is a continuous injection from $\mathcal{P}_{0}^{1}$ in to a bounded subset of $\mathbf{T}(E)$,
    \item is universal to $C_{b}(\mathcal{P}_{0}^{1},\R)$, equipped with the strict topology, 
    \item is characteristic to the space of finite regular Borel measures on $\mathcal{P}_{0}^{1}$.
  \end{enumerate}
\end{theorem}  
\begin{proof}
  See Proposition 4.1 in \cite{chevyrev2018signature}.
\end{proof}
\begin{corollary}\label{equalequal}
  Let $X$ a stochastic process on $[0,1]$ and measurable $(\Omega,\mathcal{F})$. Let $\P$ and $\mathbb{Q}$ be two regular probability measures such that $X \in \mathcal{P}_{0}^{1}$ almost surely. Then 
  \begin{equation*}
    \E_{\P}[\Phi(X)] = \E_{\mathbb{Q}}[\Phi(X)]\quad\text{iff}\quad \P = \mathbb{Q}
  \end{equation*} 
\end{corollary}
\begin{proof}
 Proof directly from (iii) of Theorem \ref{moment_law}.
\end{proof}
\begin{remark}
  If we consider the regular probability measure on time augmented path space $\mathcal{C}^{1}_{0}([0,T],\overline{E})$, similar result holds since $\mathcal{C}^{1}_{0}([0,T],\overline{E}) = \mathcal{P}_{0}^{1}$.
\end{remark}

\begin{definition}[Maximum mean distance]
  We define the maximum mean distance (MMD) as 
  \begin{equation*}
    d_{\mathcal{G}}(\P,\mathbb{Q}) = \sup_{f\in\mathcal{G}}\lvert \E_{\P}[f] - \E_{\mathbb{Q}}[f]\rvert 
  \end{equation*}
  where $\mathcal{G} \subseteq \R^{\mathcal{P}_{0}^{1}}$.
\end{definition}
Let $ \mathcal{G} = \{\langle\mathbf{l},\Phi(\cdot)\rangle_{\mathbf{T}_{1}} \colon \mathbf{l} \in \mathbf{T}_{1}, \lVert \mathbf{l}\rVert_{\mathbf{T}_{1}} \leq 1\}$, then by Corollary \ref{equalequal}
\begin{equation*}
  d_{\mathcal{G}}(\P,\mathbb{Q}) = 0\quad\text{iff}\quad \P = \mathbb{Q}
\end{equation*}
which implies that  a metric, see \cite{1993Probability}. Moreover 
\begin{equation*}
  \begin{split}
    d_{\mathcal{G}}(\P,\mathbb{Q}) &= \sup_{f\in\mathcal{G}}\lvert \E_{\P}[f] - \E_{\mathbb{Q}}[f]\rvert \\
    &= \sup_{f\in\mathcal{G}}\Big\lvert \E_{(X,Y) \sim \P\otimes\mathbb{Q}}[f(X) - f(Y)]\Big\rvert \\  
    &= \sup_{\mathbf{l} \in \mathbf{T}_{1}, \lVert \mathbf{l}\rVert_{\mathbf{T}_{1}} \leq 1}\Big\lvert \E_{(X,Y) \sim \P\otimes\mathbb{Q}}\big[\langle \mathbf{l}, \Phi(X) - \Phi(Y)\rangle_{\mathbf{T}_{1}}\big]\Big\rvert \\  
  \end{split}
\end{equation*}
Since $\E_{(X,Y) \sim \P\otimes\mathbb{Q}}\big[\Phi(X) - \Phi(Y)\big] \in \mathbf{T}_{1}$, then 
\begin{equation*}
  \begin{split}
    d_{\mathcal{G}}(\P,\mathbb{Q}) &= \E\big[\langle \Phi(X) - \Phi(Y), \Phi(X^{\prime}) - \Phi(Y^{\prime})\rangle_{\mathbf{T}_{1}}\big]\\
  \end{split}
\end{equation*}
where $X,Y,X^{\prime},Y^{\prime}$ are independent with $X,X^{\prime}\sim\P$ and $Y,Y^{\prime}\sim\mathbb{Q}$
\begin{definition}
  We denote $\mathbf{k_{Sig}}\colon \mathbf{T}_{1}\times\mathbf{T}_{1} \to \R$ the signature kernel such that 
  \begin{equation*}
    \mathbf{k_{Sig}}(\cdot,\cdot) = \langle \Phi(\cdot),\Phi(\cdot)\rangle_{\mathbf{T}_{1}}
  \end{equation*}
\end{definition}
Then we can rewrite the MMD as 
\begin{equation*}
   d_{\mathcal{G}}(\P,\mathbb{Q}) = \E\big[\mathbf{k_{Sig}}(X,X^{\prime})\big] - 2\E\big[\mathbf{k_{Sig}}(X,Y)\big] + \E\big[\mathbf{k_{Sig}}(Y,Y^{\prime})\big]
\end{equation*}
where $X,Y,X^{\prime},Y^{\prime}$ are independent with $X,X^{\prime}\sim\P$ and $Y,Y^{\prime}\sim\mathbb{Q}$. Similar to the kernel trick in machine learning, the kernel representation of MMD provides a very efficient way evaluating the metric, which we will elaborate in the next section.  





\section{Discrete signature feature}
Let $X \in \mathcal{C}^{1}_{0}([0,1],E)$ and consider a discrete sequence $\mathbf{X} = (X_{0},X_{1/N},\cdots, X_{1}) \in E^{N+1}$. We extend the concept of signature on discrete sequence $E^{N+1}$ by computing signature of the linear interpolation of sequence. From Example \ref{linear_chern} we know the signature of piecewise-linear path is 
\begin{equation*}
  \mathbf{exp}(\mathbf{X}_{1} - \mathbf{X}_{0}) \otimes \cdots \otimes \mathbf{exp}(\mathbf{X}_{N} - \mathbf{X}_{N-1})
\end{equation*}
Therefore we define the discrete signature as follows 
\begin{definition}
  Let $\mathbf{X} = (\mathbf{X}_{i})_{i=0}^{N} \in E^{N+1}$ and let $\triangle \mathbf{X}_{i} = \mathbf{X}_{i} - \mathbf{X}_{i-1}$. We denote $\mathbf{sig}_{[0,N]}$ the discrete signature of order $m$ and depth $n$ such that 
  \begin{equation*}
    \mathbf{sig}_{[0,N]}(\mathbf{X}) = \pi_{n}\bigg(\bigotimes_{i=1}^{N}\pi_{m}\Big(\mathbf{exp}\big(\triangle \mathbf{X}_{i}\big)\Big)\bigg)
  \end{equation*} 
\end{definition}
In particular, if $m = 1$ we call it growing discrete signature
  \begin{equation*}
    \begin{split}
      \mathbf{sig}_{[0,N]}(\mathbf{X}) &= \pi_{n}\bigg(\prod_{i=1}^{N}\big(1 + \triangle \mathbf{X}_{i}\big)\bigg)\\
      &= \sum_{k=0}^{n}\sum_{i_{1} < \cdots < i_{k} = 1}^{N} \triangle \mathbf{X}_{i_{1}} \otimes \cdots \otimes \triangle \mathbf{X}_{i_{k}}.
    \end{split}
  \end{equation*} 
  If $m > 1$, we call it high order discrete signature, and if $m \geq n$, we call it truncated discrete signature
  \begin{equation*}
    \begin{split}
      \mathbf{sig}_{[0,N]}(\mathbf{X}) &= \pi_{n}\bigg(\prod_{i=1}^{N}\mathbf{exp}\big(\triangle \mathbf{X}_{i}\big)\bigg)\\
      &= \sum_{k=0}^{n}\sum_{i_{1} \leq \cdots \leq i_{k} = 1}^{N} \triangle \mathbf{X}_{i_{1}} \otimes \cdots \otimes \triangle \mathbf{X}_{i_{k}} 
    \end{split}
  \end{equation*} 
  \begin{lemma}
    Let $X \in \mathcal{C}^{1}_{0}([0,1],E)$ and $\mathbf{X} = (X_{i/N})_{i=0}^{N} \in E^{N+1}$, then
    \begin{equation*}
      \lVert \mathbf{Sig}_{[0,1]}(X) - \mathbf{sig}_{[0,N]}(\mathbf{X})\rVert_{\mathbf{T}_{1}} \to 0\quad\text{as}\quad N \to 0
    \end{equation*}
  \end{lemma}
  \begin{proof}
    See Theorem 5 in \cite{kiraly2019kernels}.
  \end{proof}
  \begin{remark}
    If we let $X$ to be the Brownian motion (not in $\mathcal{C}^{1}_{0}$), the growing discrete signature converges to the signature defined with Ito integral, while the high order discrete signature converges to the signature defined with Stratonovich integral. More generally, if we want to approximate a geometric $p$-rough path, we need high order discrete siganture at least order $\lfloor p \rfloor$, see \cite{kiraly2019kernels}.
  \end{remark}
  \subsection{Kernel trick}
  In the same fashion of horner algorithm 
  \begin{equation*}
    \begin{aligned}a_{0}&+a_{1}x+a_{2}x^{2}+a_{3}x^{3}+\cdots +a_{n}x^{n}\\&=a_{0}+x{\bigg (}a_{1}+x{\Big (}a_{2}+x{\big (}a_{3}+\cdots +x(a_{n-1}+x\,a_{n})\cdots {\big )}{\Big )}{\bigg )}.\end{aligned}
  \end{equation*}
  we can reduce the computational complexity of discrete signature by dynamic programming principle.
  \begin{proposition}Let $\mathbf{X} = (\mathbf{X}_{i})_{i=0}^{N} \in E^{N+1}$ and let $\mathbf{sig}_{[0,N]}$ be the growing signature
  \begin{equation*}
    \begin{split}
      \mathbf{sig}_{[0,N]}(\mathbf{X}) &= \mathbf{sig}_{[0,N-1]}(\mathbf{X}) \otimes(1 + \triangle \mathbf{X}_{N})\\
      &= 1 + \sum_{i_{1} = 1}^{N}\triangle \mathbf{X}_{i_{1}}\Big(1 + \sum_{i_{2} = i_{1}+1}^{N}\triangle\mathbf{X}_{i_{2}}(\cdots)\Big)\\
    \end{split}
  \end{equation*}
  \end{proposition}
  This is too surprise because we just rewrite the Chen's identity. However, this greatly reduce the complexity when computing the kernel. 
  \begin{definition} Let $\mathbf{sig}_{[0,N]}$ be the discrete signature, then we denote $\mathbf{k}_{\mathbf{sig}}$ the discrete signature kernel such that 
  \begin{equation*}
    \mathbf{k}_{\mathbf{sig}}(\cdot,\cdot) = \langle \mathbf{sig}_{[0,N]}(\cdot),\mathbf{sig}_{[0,N]}(\cdot)\rangle_{\mathbf{T}_{1}}
  \end{equation*}
  \end{definition}
  \begin{proposition}\label{recur1}
    Let $\mathbf{X} = (\mathbf{X}_{i})_{i=0}^{N} \in E^{N+1}$ and let $\mathbf{Y} = (\mathbf{Y}_{i})_{i=0}^{N} \in E^{N+1}$. Let $\mathbf{k}_{\mathbf{sig}}$ be the kernel of discrete growing signature
  \begin{equation}\label{kernel}
    \mathbf{k}_{\mathbf{sig}}(\mathbf{X},\mathbf{Y}) = 1 + \sum_{\substack{i_{1}=1 \\ j_{1} = 1}}^{N}\langle \triangle \mathbf{X}_{i_{1}},  \triangle \mathbf{Y}_{j_{1}} \rangle\Big(1 + \sum_{\substack{i_{2}=i_{1}+1 \\ j_{2} = j_{1}+1}}^{N}\langle \triangle \mathbf{X}_{i_{2}},  \triangle \mathbf{Y}_{j_{2}} \rangle(\cdots)\Big)
  \end{equation}
  \end{proposition}  
  \begin{remark}
    Similar formulas hold for high order discrete signature and kernel, see \cite{kiraly2019kernels}. 
  \end{remark}
  This result is important for numerical implementation in two aspects. First is that we avoid computing the tensor but the inner product directly by kernel trick. If somehow (for better characteristic capacity) you would like lift paths to a RKHS space $(H,\kappa)$ before applying the signature kernel, this formula avoids explicitly compute $\triangle\kappa_{X}$ and $\triangle\kappa_{Y}$. but approximating with
    \begin{equation*}
      \langle\triangle\kappa_{X_{i}}, \triangle\kappa_{Y_{j}}\rangle\approx\kappa(X_{i+1},Y_{j+1}) + \kappa(X_{i},Y_{j}) - \kappa(X_{i},Y_{j+1}) - \kappa(X_{i+1},Y_{j}),
    \end{equation*}
    which reduce a potentially infinite computational complexity if $(H,\kappa)$ is an infinite dimensional space, see \cite{chevyrev2018signature}. Secondly, the recursive structure in time implies that computing the inner product of signature stream is as cheap as computing the inner product of signature. This recursive feature can also be used for more general inner product between tensor and discrete signature. 

\section{Low-rank-signature}
We extend the recursive method from discrete signature to tensor with similar recursive structure. Let $\mathbf{l} = (\mathbf{l}_{k})_{k=0}^{\infty} \in \mathbf{T}_{1}(E)$, let $\mathbf{X} = (\mathbf{X}_{i})_{i=0}^{N} \in E^{N+1}$,and let $\mathbf{sig}_{[0,N]}$ be the growing signature. If $\mathbf{l}_{k} = \mathbf{l}_{k-1}\otimes l_{k}$ then
  \begin{equation*}
    \begin{split}
     \langle \mathbf{l}, \mathbf{sig}_{[0,N]}(\mathbf{X}) \rangle_{\mathbf{T}_{1}(E)} &= \sum_{k\geq 0}\langle \mathbf{l}_{k} , \mathbf{sig}^{(k)}_{[0,N]}(\mathbf{X}) \rangle_{\mathbf{T}_{1}(E)} \\
     &= \sum_{k\geq 0}\sum_{i=1}^{N} \langle \mathbf{l}_{k-1} ,\mathbf{sig}^{(k-1)}_{[0,i-1]}(\mathbf{X})\rangle_{\mathbf{T}_{1}(E)}\cdot\langle l_{k},\triangle \mathbf{X}_{i}\rangle_{\mathbf{T}_{1}(E)} \\
     &= 1 + \sum_{\substack{i_{1}=1 \\ j_{1} = 1}}^{N}\langle l_{i_{1}},  \triangle \mathbf{X}_{j_{1}} \rangle\Big(1 + \sum_{\substack{i_{2}=i_{1}+1 \\ j_{2} = j_{1}+1}}^{N}\langle l_{i_{2}},  \triangle \mathbf{X}_{j_{2}} \rangle(\cdots)\Big)
    \end{split}
  \end{equation*}
  This is exactly the formula \eqref{kernel} in Proposition \ref{recur1}. In the intermediate step of computing the inner product, we have obtained the value of $\langle \mathbf{l}_{k} , \mathbf{sig}^{(k)}_{[0,N]}(\mathbf{X}) \rangle_{\mathbf{T}_{1}(E)}$ for all $k$. Therefore, if we consider a sequence of $L$ many tensors such that $\mathbf{l}_{k} = \mathbf{l}_{k-1}\otimes l_{k}$, then we can leverage the recursive structure in order and compute their inner product with signature in linear computational complexity w.r.t $L$.
  \begin{remark}
    This result can also be generalized to higher order case by considering the recursive algorithm computing high order discrete signature and kernel, see \cite{kiraly2019kernels}. However, in the paper \cite{toth2020seq2tens} introducing low-rank tensor projection of ordered sequential data, only first order recursive structure is considered. We believe that higher order recursive structure also exists and we wish to prove this claim in further research. 
  \end{remark} 
  
  If we only assume $\mathbf{l}_{k} = l_{k,1} \otimes \cdots \otimes l_{k,k}$, then we lose the recursive in order, but still we have the recursive in time of discrete signature and the kernel trick.
    \begin{equation*}
    \begin{split}
     \langle \mathbf{l}_{k}, \mathbf{sig}^{(k)}_{[0,N]}(\mathbf{X}) \rangle_{\mathbf{T}_{1}(E)} &= \sum_{i=1}^{N} \langle l_{k,1} \otimes \cdots \otimes l_{k-1,k} ,\mathbf{sig}^{(k-1)}_{[0,i-1]}(\mathbf{X})\rangle_{\mathbf{T}_{1}(E)}\cdot\langle l_{k,k},\triangle \mathbf{X}_{i}\rangle_{\mathbf{T}_{1}(E)} \\
    \end{split}
  \end{equation*}
  
  \begin{definition}
    We call $\mathbf{l}  \in \mathbf{T}_{1}$ rank-1 tensor if
    \begin{equation*}
      \mathbf{l} = l_{1} \otimes \cdots \otimes l_{k},\quad \text{for some  }k \geq 1
    \end{equation*}
    % and we call a set of rank-1 tensor $L = \{\mathbf{l} \in \mathbf{T}_{1}\colon\mathbf{l}\text{ is rank-1}\}$ recursive tensors if for all $\mathbf{l} = l_{1} \otimes \cdots \otimes l_{k} \in L$ 
    % \begin{equation*}
    %   l_{1} \otimes \cdots \otimes l_{i} \in L,\quad \forall i = 1,\cdots,k
    % \end{equation*} 
  \end{definition}
  Similar to log-signature, the inner product between truncated signature and rank-1 tensor map the signature space to a low dimensional space $\R$. This projection is linear, computational cheap, and more importantly trainable. The choice of rank-1 tensors gives us the freedom to choose the output space dimension as well as the direction of projection.
  \begin{definition}
    Let $(\mathbf{l}_{l})_{l=0}^{L}$ a sequence of rank-1 tensors in $\mathbf{T_{1}}$ and let $\mathbf{sig}_{[0,N]}$ the discrete signature, then we denote $\mathbf{LRsig}_{[0,N]}$ the low-rank signature such that 
  \begin{equation*}
    \mathbf{LRsig}_{[0,N]} = \big(\langle \mathbf{l}_{l} , \mathbf{sig}_{[0,N]} \rangle_{\mathbf{T}_{1}(E)}\big)_{l=0}^{L} \in \R^{L}
  \end{equation*} 
  and we define the low-rank signature stream $(\mathbf{LRsig}_{[0,i]})_{i=0}^{N}$.
  \end{definition}
  From the recursive algorithm above, we know that computing the low-rank signature stream is as cheap as computing the low-rank signature. Unlike signature stream, low-rank signature stream still in a low dimensional space, so we may again apply the low-rank signature stream to it. Thus, we may stack many levels of signature stream on the original path and this has been proved successful dealing with sequential data, see \cite{toth2020seq2tens}.

  
\chapter{Variational Autoencoders}
In this chapter, we give a brief introduction to the variational autoencoders (VAEs) and some variations of VAE such as conditional VAEs (CVAEs), $\beta-$VAEs, student-VAEs, and gaussian process VAEs (GP-VAEs). Variational autoencoders is a generative model which simulates how the data is generated under certain presumed distribution. In short, VAEs are variational bayesian inference on latent variable models with likelihood and posterior described by neural networks. In the scenario of generating time series, there are in general two frameworks to apply variational autoencoders. First is to assume a bayesian model with both observable variables and latent variables taking values in space of time series, for example taking latent space as a gaussian process in GP-VAEs. Another approach is to first consider an appropriate feature map from time series to a finite dimensional features space and apply VAE on this feature space. After the training of VAE on features space, we generate new samples on the feature space and transform those samples on feature space back to time series with the inverse map of the feature map. More details and background information on VAEs can be found
in 
\section{Variational autoencoders}
Variational autoencoders are variational inference on deep latent variable model. Let consider deep latent variable models consists of observable random variable $x$, latent random variable $z$, and parameter $\theta$. The joint distribution $p_{\theta}(x,z)$ of the deep latent variable model is parametrized by neural networks. For example, we may choose $z$ be standard normal distribution and $x$ a normal distribution with mean $\mu = \mathbf{NN}(z)$ and variance $1$, where $\mathbf{NN}$ is a neural network. Moreover, we call $p_{\theta}(z) = p(z)$ the \textit{prior} (independent with $\theta$), $p_{\theta}(z|x)$ the \textit{posterior}, $p_{\theta}(x|z)$ the \textit{conditional distribution} and $p_{\theta}(x)$ the \textit{marginal likelihood}. The big advantage of deep latent variable model is that even when the prior $p(z)$ and conditional distribution $p(x|z)$ are explicit and simple, the marginal likelihood $p_{\theta}(x)$ can be very expressive due to the universal approximating capacity of neural network. However, the price to pay for expressive marginal likelihood is the intractability of marginal likelihood, namely having no analytic solution or efficient estimator for it. By Bayes' rule 
$$p_{\theta}(x) = \frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}.$$
The joint distribution is not hard to write down since 
$$p_{\theta}(x,z) = p\big(x|\mathbf{NN}_{\theta}(z)\big) p(z)$$
but the posterior $p_{\theta}(z|x)$ in general has no analytic solution or efficient estimator. Therefore, we need to leverage the idea of variational inference, introducing a distribution $q_{\phi}(z|x)$ approximating the true posterior $p_{\theta}(z|x)$.
\subsection{Variational inference}
Let $x_{1},\cdots, x_{n}$ be i.i.d. observable samples drawn from a random variable $x$. Maximizing the log-marginal likelihood by Bayes's rule, we obtain
\begin{equation*}
  \begin{split}
    \log p_{\theta}(x_{i}) &= \log\Big( \frac{p_{\theta}(x_{i},z)}{p_{\theta}(z|x_{i})}\Big)\\
    &= \log\Big( \frac{p_{\theta}(x_{i},z)}{q_{\phi}(z|x_{i})} \frac{q_{\phi}(z|x_{i})}{p_{\theta}(z|x_{i})}\Big)\\
    &= \E_{q_{\phi}(z|x_{i})}\bigg[\log\Big( \frac{p_{\theta}(x_{i},z)}{q_{\phi}(z|x_{i})}\Big) + \log\Big( \frac{q_{\phi}(z|x_{i})}{p_{\theta}(z|x_{i})}\Big)\bigg]\\
    &= \E_{q_{\phi}(z|x_{i})}\bigg[\log\Big( \frac{p_{\theta}(x_{i},z)}{q_{\phi}(z|x_{i})}\Big) + \log\Big( \frac{q_{\phi}(z|x_{i})}{p_{\theta}(z|x_{i})}\Big)\bigg]\\
    &= \E_{q_{\phi}(z|x_{i})}\bigg[\log\Big( \frac{p_{\theta}(x_{i},z)}{q_{\phi}(z|x_{i})}\Big)\bigg] + KL\big(q_{\phi}(z|x_{i}) \big|\big| p_{\theta}(z|x_{i})\big)\\
    &\geq \E_{q_{\phi}(z|x_{i})}\bigg[\log\Big( \frac{p_{\theta}(x_{i},z)}{q_{\phi}(z|x_{i})}\Big)\bigg]
  \end{split}
\end{equation*}
Let us denote that $\mathcal{L}_{\theta, \phi}(x)$ the \textit{evidence lower bound} (ELBO) such that 
\begin{equation*}
  \mathcal{L}_{\theta, \phi}(x) = \E_{q_{\phi}(z|x)}\bigg[\log\Big( \frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\Big)\bigg]
\end{equation*}
Notice that 
\begin{equation*}
  \log p_{\theta}(x) = \max_{\phi} \mathcal{L}_{\theta, \phi}(x).
\end{equation*}
Thus, we can solve the maximization problem of ELBO instead of the marginal likelihood. Also if 
\begin{equation*}
  q_{\phi}^{*}(z|x) \in \argmax\mathcal{L}_{\theta, \phi}(x)
\end{equation*}
then it satisfies that $q_{\phi}^{*}(z|x) = p _{\theta}(z|x)$ because
\begin{equation*}
  KL\big(q_{\phi}(z|x_{i}) \big|\big| p_{\theta}(z|x_{i})\big) = 0 \quad \text{iff} \quad q_{\phi}(z|x) = p _{\theta}(z|x).
\end{equation*}
Therefore maximizing the ELBO also gives us the true posterior.
Moreover, observe that 
\begin{equation*}
  \begin{split}
    \mathcal{L}_{\theta, \phi}(x) &= \E_{q_{\phi}(z|x)}\bigg[\log\Big( \frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\Big)\bigg]\\
    &= \E_{q_{\phi}(z|x)}\bigg[\log\Big( \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\Big)\bigg]\\
    &= -KL\big(q_{\phi}(z|x) \big|\big| p_{\theta}(z)\big) + \E_{q_{\phi}(z|x)}\big[p_{\theta}(x|z)\big].\\
  \end{split}
\end{equation*}
Thus, we are minimizing the KL-distance between approximating posterior and true prior, meanwhile maximizing expected conditional distribution under approximating posterior distribution. 
\subsection{Reparameterization Trick}
In order the solve the variational problem, we compute the gradient of the objective function w.r.t $\theta$ and $\phi$. However, the gradient w.r.t $\phi$ is in general hard to compute because the expectation also depends on $\phi$.
\begin{equation*}
  \begin{split}
      \nabla_{\phi}\mathcal{L}_{\theta, \phi}(x) &= \nabla_{\phi} \E_{q_{\phi}(z|x)}\big[\log p_{\theta}(x,z)- \log q_{\phi}(z|x) \big] \\ 
      &\neq  \E_{q_{\phi}(z|x)}\big[\nabla_{\phi}(\log p_{\theta}(x,z)- \log q_{\phi}(z|x) )\big] \\ 
  \end{split}
\end{equation*}
Therefore, we need to factorize the randomness taking expectation out of the parameter $\phi$. So we consider 
\begin{equation*}
  z = g(\epsilon,\phi,x)
\end{equation*}
where $g$ is a differentiable function and $\epsilon$ is independent with $\phi$ and $x$. In this case, for all differentiable function $f$ 
\begin{equation*}
  \begin{split}
    \nabla_{\phi}\mathcal{L}_{\theta, \phi}(x)&= \nabla_{\phi} \E_{q_{\phi}(z|x)}\Big[f(x,z)\Big]\\
    &= \nabla_{\phi} \E_{g(\epsilon,\phi,x))}\Big[f(x,z)\Big]\\
    &= \nabla_{\phi} \E_{\epsilon}\Big[f\big(x,g(\epsilon,\phi,x)\big)\Big]\\
    &= \E_{\epsilon}\Big[\nabla_{\phi} f\big(x,g(\epsilon,\phi,x)\big)\Big]\\
  \end{split}
\end{equation*}
\begin{example}
  Consider a deep latent variable model with gaussian prior and gaussian conditional distribution:
  \begin{equation*}
    \begin{split}
      q_{\phi}(z|x) &= \mathcal{N}\Big(z;\mu_{x},\sigma_{x}^{2}\Big)\\
      p_{\theta}(z) &= \mathcal{N}\Big(z;0,I_{d}\Big)
    \end{split}
  \end{equation*}
  where $\mu_x \in \R^{d}$ and $\sigma_{x}^{2}$ be a diagonal matrix in $\R^{d\times d}$.
  Then 
  \begin{equation*}
    \begin{split}
       -KL\big(q_{\phi}(z|x) \big|\big| p_{\theta}(z)\big) &= \E_{q_{\phi}(z|x)}\big[\log p_{\theta}(z)\big] - \E_{q_{\phi}(z|x)}\big[\log q_{\phi}(z|x)\big]\\
       &= \int \log p_{\theta}(z) q_{\phi}(z|x) dz - \int \log p_{\theta}(z|x) q_{\phi}(z|x) dz\\
       &= \frac{1}{2}\sum_{i=1}^{d}\Big(1 + \log(\sigma_{i}^{2}) - \mu_{i}^{2} - \sigma_{i}^{2}\Big)
    \end{split}
  \end{equation*}
\end{example}

% If we also choose the approximating posterior $q_{\phi}(z|x)$ be a simple distribution parametrized by neural network i.e. 
% \begin{equation*}
%   q_{\phi}(z|x) = q(z | \mathbf{NN}(x))
% \end{equation*}
% where $q$ is a simple distribution such as normal distribution and $\mathbf{NN}$ a neural network. 
\section{Variations of VAE}

\chapter{Main method}
\section{Time series processing}
\section{Generative model with heavy tail}
\section{Inverse the log-signature}
\section{Evaluation with lifting and normalizing}

\chapter{Signature stimulation}
\section{Controlled stochastic differential equation}
\section{Numerical Implementation}



\chapter{Conclusions and Future Work}
\chapter{Appendix}

\nocite{*}
\bibliographystyle{acm}
\bibliography{workingthesisbib}

\end{document}


 
  